{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# GroMo tutorial\n\nThis is a minimal example of how to use GroMo. We will illustrate the\nuse off GroMo to find a one hidden layer neural network (NN) that\napproximates the function $f(x) = \\sin(x)$ on the interval\n$[0, 2\\pi]$.\n\n\n## Imports\n\n-  We use ``torch`` as the backend for all the computations.\n-  ``matplotlib`` is used for plotting the results.\n-  ``global_device`` is used to automatically select the device (CPU or\n   GPU) for computations.\n-  ``LinearGrowingModule`` is the main class of GroMo, which implements\n   fully connected growing modules.\n-  ``SinDataLoader`` is a custom data loader that generates the training\n   data for the sine function.\n-  ``train`` is exactly like a **standard PyTorch training loop**\n-  ``evaluate`` is exactly like a **standard PyTorch evaluation loop**\n\nThen we define ``plt_model`` to visualize the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nfrom helpers.auxilliary_functions import SinDataloader, evaluate_model, train\n\nfrom gromo.modules.linear_growing_module import LinearGrowingModule\nfrom gromo.utils.utils import global_device\n\n\nglobal_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plt_model(model: torch.nn.Module, fig: \"plt.axes._axes.Axes\") -> None:\n    \"\"\"\n    Plot the model's predictions and the true function.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The model to plot.\n    fig : plt.axes._axes.Axes\n        The figure to plot on.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    x = torch.linspace(0, 2 * torch.pi, 1000, device=global_device()).view(-1, 1)\n    y = torch.sin(x)\n    y_pred = model(x)\n    fig.plot(x.cpu().numpy(), y.cpu().numpy(), label=\"sin\")\n    fig.plot(x.cpu().numpy(), y_pred.cpu().detach().numpy(), label=\"Predicted\")\n    fig.legend()\n    fig.set_xlabel(\"x\")\n    fig.yaxis.set_label_position(\"right\")\n    fig.set_ylabel(\"sin(x)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = SinDataloader(nb_sample=10, batch_size=100)\nloss_function = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "first_layer = LinearGrowingModule(\n    in_features=1,\n    out_features=2,\n    use_bias=True,\n    post_layer_function=torch.nn.GELU(),\n    name=\"first_layer\",\n)\n\nsecond_layer = LinearGrowingModule(\n    in_features=2,\n    out_features=1,\n    use_bias=True,\n    name=\"second_layer\",\n    previous_module=first_layer,\n)\n\ngrowing_net = torch.nn.Sequential(\n    first_layer,\n    second_layer,\n)\n\ngrowing_net = growing_net.to(global_device())\n\nprint(growing_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define the following network:\n\n\\begin{align}x \\mapsto \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} \\mapsto \\begin{bmatrix} \\sigma(z_1) \\\\ \\sigma(z_2) \\end{bmatrix} \\mapsto y\\end{align}\n\nwhere $\\sigma$ is the activation function, $z_1$ and\n$z_2$ are the outputs of the first fully connected layer, and\n$y$ is the output of the whole network.\n\nNote that the **activation function is included in the first\n``LinearGrowingModule`` layer**, this allow to easily access the\nintermediate results both before and after the activation function.\n\nNote also that the second layer is linked to the first one by\n``previous_module=first_layer``. This allows extending the input of the\nsecond layer and let it grow with the first layer.\n\n### 2. Use it like a normal model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2_err = evaluate_model(growing_net, data, loss_function)[0]\nprint(f\"Initial error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(growing_net, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we guide it a bit in the right direction to make it learn faster\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[0].weight.data = torch.ones_like(growing_net[0].weight.data)\ngrowing_net[0].bias.data = torch.tensor([-2.0, -3 * torch.pi / 2], device=global_device())\ngrowing_net[1].weight.data = torch.tensor([[-1.0, 2.0]], device=global_device())\ngrowing_net[1].bias.data = torch.zeros_like(growing_net[1].bias.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2_err = evaluate_model(growing_net, data, loss_function)[0]\nprint(f\"Error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(growing_net, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(growing_net.parameters(), lr=1e-2)\n# optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n\nres = train(\n    model=growing_net,\n    train_dataloader=data,\n    optimizer=optimizer,\n    nb_epoch=10,\n    show=False,\n    aux_loss_function=None,\n)\nloss_train, accuracy_train, loss_val, _ = res\nplt.plot(loss_train, label=\"train\")\nplt.plot(loss_val, label=\"val\")\nplt.legend()\nplt.show()\n\nl2_err = evaluate_model(growing_net, data, loss_function, aux_loss_function=None)[0]\nprint(f\"Error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(growing_net, ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here with only two hidden neurons, we have a limited expressiveness.\nTherefore we would like to add new neurons to the model.\n\n## 3. Prepare the growth\n\nTo add new neurons we need information about the current model. To get\nthose the first set is to initialize the computation of those.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above you can see that nothing is stored in the model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].init_computation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above you can see that one the computation are initialised, we see that\n``Store input : True``. This means that the next time we forward through\nthe graph we will store the input of the layers for which\n``store_input=True``.\n\nWe then do the forward/backward pass to compute all the raw information\nneeded, then we call ``update_computation`` to aggregate the raw\ninformatons into statistics like the tensors S and M.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here we switch to a sum loss function !\n# This is important as we already make the average internally\nloss_sum = torch.nn.MSELoss(reduction=\"sum\")\n\nfor x, sinx in data:\n    out = growing_net(x)\n\n    error = loss_sum(out, sinx)\n    error.backward()\n\n    growing_net[1].update_computation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below you can see that indeed ``Tensor S`` and ``Tensor M`` are now\nestimated over 1000 samples.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can compute the natural gradient step and the new neurons to add\nwith ``compute_optimal_updates``. You can see that now the first layer\nstore an extended output layer that will compute the new values of the\nneurons. The second layer has also been extended with an extended input\nlayer. In addition the second layer has a ``Optimal delta layer`` which\ncorrespond to the natural gradient step.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].compute_optimal_updates()\nprint(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the updates are computed we can stop computing statistics and\nstoring them. This is done by calling the ``reset_computation`` method.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].reset_computation()\nprint(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that the first layer still store the input. To correct\nit we can simply set ``store_input=False`` in the first layer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[0].store_input = False\nprint(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Choose a scaling factor\n\nOnce we computed the updates we can choose a scaling factor. This\nscaling factor $\\gamma$ will scale the updates by $\\gamma$\nwhen they have a quadratic effect (like the new incoming and outgoing\nweights) or $\\gamma^2$ when they have a linear effect (like the\nnatural gradient step).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].scaling_factor = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def extended_evaluate_model(\n    growing_model: torch.nn.Sequential,\n    dataloader: torch.utils.data.DataLoader,\n    loss_function: torch.nn.Module = torch.nn.MSELoss(reduction=\"sum\"),\n    batch_limit: int = -1,\n    device: torch.device = global_device(),\n) -> float:\n    assert (\n        loss_function.reduction == \"sum\"\n    ), \"The loss function should not be averaged over the batch\"\n    growing_model.eval()\n    n_batch = 0\n    nb_sample = 0\n    total_loss = torch.tensor(0.0, device=device)\n    for x, y in dataloader:\n        growing_model.zero_grad()\n        x, y = x.to(device), y.to(device)\n        z, z_ext = growing_model[0].extended_forward(x)\n        y_pred, _ = growing_model[1].extended_forward(z, z_ext)\n        loss = loss_function(y_pred, y)\n        total_loss += loss\n        nb_sample += x.size(0)\n        n_batch += 1\n        if 0 <= batch_limit <= n_batch:\n            break\n    return total_loss.item() / nb_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a special forward ``extended_forward`` that takes into\naccount the proposed modification of the network to evaluate their\neffect on the loss.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_error = extended_evaluate_model(\n    growing_net,\n    data,\n    loss_sum,\n)\n\nprint(f\"New error: {new_error:.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Apply the changes\n\nOnce we have chosen a scaling factor we can apply the changes to the\nmodel. This is done by calling the ``apply_change`` methods.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].apply_change()\nprint(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then delete the ``extended_output_layer``,\n``extended_input_layer`` and ``Optimal delta layer`` as they are not\nneeded anymore.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growing_net[1].delete_update()\nprint(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Use your grown model\n\nYou then get a fully working model that can be used like a normal\nPyTorch model. You can train it, evaluate it, etc.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l2_err = evaluate_model(growing_net, data, loss_function)[0]\nprint(f\"New error: {l2_err:.2e}\")\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\nplt_model(growing_net, ax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(growing_net[0].__str__(verbose=2))\nprint(growing_net[1].__str__(verbose=2))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
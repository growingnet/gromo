{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# GrowingContainer tutorial\n\nA step-by-step guide to neural network growing using the **GroMo** (Growing\nModules) library.\n\n## What is GroMo?\n\nGroMo is a library that enables **progressive neural network growth** during\ntraining. Instead of defining a fixed architecture upfront, you start with a\nsmall network and **dynamically add neurons** to layers based on gradient\ninformation. This approach can lead to **Informed growth**: New neurons are\nadded in directions that most reduce the loss.\n\n## Tutorial Overview\n\nIn this notebook, we will:\n\n1. Set up the environment\n2. Create a small ``GrowingMLP`` model\n3. Define training, evaluation, and growth functions\n4. Iteratively train and grow the network\n\nThis process is presented for the ``GrowingMLP`` structure but it can be\ngeneralized to other structures like a ResNet, a MLP Mixer, etc.\n\nLet's get started!\n\n## Step 1: Environment Setup and Imports\n\nFirst, we import the necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport torch\nimport torch.utils.data\nfrom helpers.synthetic_data import MultiSinDataloader\n\nfrom gromo.containers.growing_container import GrowingContainer\nfrom gromo.containers.growing_mlp import GrowingMLP\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Define the data loaders\n\nWe use a custom dataloader with synthetic data for training and testing.\n\nThe input $x \\sim \\mathcal{N}(0_k, 1_k)$ and the target is defined as:\n\n\\begin{align}y[d] = \\sum_{i=1}^{k} \\sin(i x[i] + d)\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "in_features = 10\nout_features = 3\n\ntrain_data_loader = MultiSinDataloader(\n    nb_sample=10,\n    batch_size=1_000,\n    in_features=in_features,\n    out_features=out_features,\n    seed=0,\n    device=device,\n)\n\ntest_data_loader = MultiSinDataloader(\n    nb_sample=1,\n    batch_size=1_000,\n    in_features=in_features,\n    out_features=out_features,\n    seed=1,\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the GrowingMLP Architecture\n\nBefore we use ``GrowingMLP``, let's look at its\n[implementation](https://github.com/growingnet/gromo/blob/main/src/gromo/containers/growing_mlp.py)\nto understand how it works.\n\n#### Key Components\n\n- **LinearGrowingModule**: A special linear layer that supports dynamic\n  growth. Each layer can have neurons added to it during training.\n\n- **GrowingContainer**: The base class that provides the growth infrastructure\n\n#### Important Methods\n\n+-------------------------------+----------------------------------------+\n| Method                        | Description                            |\n+===============================+========================================+\n| ``__init__``                  | Define the layers, carefully link them |\n|                               | together                               |\n+-------------------------------+----------------------------------------+\n| ``forward(x)``                | Standard forward pass using current    |\n|                               | weights                                |\n+-------------------------------+----------------------------------------+\n| ``extended_forward``          | Forward pass that includes proposed    |\n|                               | new neurons                            |\n+-------------------------------+----------------------------------------+\n| ``set_growing_layers(index)`` | Select which layer(s) to grow          |\n+-------------------------------+----------------------------------------+\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Define Helper Functions\n\nWe need three key functions to work with our growing network:\n\n1. **Training function**: Standard training loop using SGD\n2. **Evaluation function**: Compute loss, with support for extended mode\n3. **Growth function**: The core GroMo logic to add new neurons intelligently\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Training Function\n\nA standard PyTorch training loop that performs one epoch of training using\nSGD optimizer with MSE loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def train(\n    model: torch.nn.Module,\n    device: torch.device,\n    train_loader: torch.utils.data.DataLoader,\n) -> None:\n    \"\"\"\n    Train the model for one epoch using SGD optimizer.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The neural network model to train.\n    device : torch.device\n        The device (CPU or CUDA) to run computations on.\n    train_loader : torch.utils.data.DataLoader\n        DataLoader providing batches of (input, target) pairs.\n    \"\"\"\n    model.train()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    criterion = torch.nn.MSELoss()\n\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Evaluation Function\n\nThe evaluation function computes the loss on a dataset. It supports two\nmodes:\n\n- **Standard mode** (``extended=False``): Uses the current model weights\n- **Extended mode** (``extended=True``): Evaluates the model *as if* the\n  proposed new neurons were added\n\nThis allows us to preview the effect of growth before committing to it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\ndef evaluate(\n    model: GrowingContainer,\n    device: torch.device,\n    test_loader: torch.utils.data.DataLoader,\n    extended: bool = False,\n) -> float:\n    \"\"\"\n    Evaluate the model on a dataset.\n\n    Parameters\n    ----------\n    model : GrowingContainer\n        The neural network model to evaluate.\n    device : torch.device\n        The device (CPU or CUDA) to run computations on.\n    test_loader : torch.utils.data.DataLoader\n        DataLoader providing batches of (input, target) pairs.\n    extended : bool, optional\n        If True, use extended_forward which includes proposed new neurons.\n        If False, use standard forward pass. Default is False.\n\n    Returns\n    -------\n    loss : float\n        The average mean squared error loss per sample.\n    \"\"\"\n    model.eval()\n    criterion = torch.nn.MSELoss(reduction=\"mean\")\n    loss = 0.0\n\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        if extended:\n            output = model.extended_forward(data)\n        else:\n            output = model(data)\n        loss += criterion(output, target).item()\n\n    loss /= len(test_loader)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Growth Function\n\nThis is the **core of GroMo** - the function that intelligently grows the\nnetwork. Here's what happens step by step:\n\n1. **set_growing_layers(layer_to_grow)**: Specify which layer(s) will be grown\n2. **init_computation()**: Initialize internal buffers to accumulate gradient\n   statistics\n3. **Forward/backward pass loop**: Process the entire dataset, accumulating\n   information about optimal growth directions via ``update_computation()``\n4. **compute_optimal_updates()**: Solve for the optimal new neuron weights\n   based on accumulated statistics\n5. **dummy_select_update()**: Here it's trivial as we selected the layer\n   before computing the statistics but we could have done it the other way\n   around and select the layer to grow by looking at the different proposed\n   updates\n6. **Line search**: Try different scaling factors to find the best magnitude\n   for the new neurons\n7. **apply_change()**: Permanently add the new neurons to the network\n\nThe **line search** is crucial: it determines how much to \"trust\" the\ncomputed optimal neurons. A scaling factor of 0 means no growth, while\nlarger values add stronger contributions from new neurons.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def grow(\n    model: GrowingMLP,\n    device: torch.device,\n    train_loader: torch.utils.data.DataLoader,\n    layer_to_grow: int,\n) -> None:\n    \"\"\"\n    Grow the specified layer of the model by adding new neurons.\n\n    Parameters\n    ----------\n    model : GrowingMLP\n        The neural network model to grow.\n    device : torch.device\n        The device (CPU or CUDA) to run computations on.\n    train_loader : torch.utils.data.DataLoader\n        DataLoader providing batches of (input, target) pairs.\n    layer_to_grow : int\n        Index of the layer to grow (1-indexed).\n\n    Notes\n    -----\n    The growth procedure:\n    1. Accumulate gradient statistics over the training set\n    2. Compute optimal weights for new neurons\n    3. Find the best scaling factor via line search\n    4. Apply the growth to the model\n    \"\"\"\n    model.eval()\n    # /!/ We use the reduction=\"sum\" as the averaging is already\n    # done in the GrowingMLP methods\n    criterion = torch.nn.MSELoss(reduction=\"sum\")\n\n    model.set_growing_layers(layer_to_grow)\n    model.init_computation()\n\n    for data, target in train_loader:\n        data, target = data.to(device), target.to(device)\n        model.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        model.update_computation()\n\n    model.compute_optimal_updates()\n    model.reset_computation()\n\n    model.dummy_select_update()\n\n    # Line search to find the best scaling factor\n    best_loss = float(\"inf\")\n    best_value = 0.0\n    for value in [0.0, 0.1, 0.5, 1.0]:\n        model.set_scaling_factor(value)\n        loss = evaluate(model, device, train_loader, extended=True)\n        print(f\"Scaling factor: {value}, Loss: {loss:.4f}\")\n        if loss < best_loss:\n            best_loss = loss\n            best_value = value\n    print(f\"Best scaling factor: {best_value}, loss: {best_loss:.4f}\")\n\n    model.set_scaling_factor(best_value)\n    model.apply_change()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 4: Create the Initial Model\n\nNow we create our ``GrowingMLP`` model with:\n\n- **Input size**: 10 (features)\n- **Output size**: 3 (targets)\n- **Hidden size**: 2 neurons (intentionally small - we'll grow it!)\n- **Number of hidden layers**: 2\n\nStarting with such a small network demonstrates how GroMo can grow the\narchitecture from minimal capacity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number_hidden_layers = 2\n\ntorch.manual_seed(0)\nmodel = GrowingMLP(\n    in_features=in_features,\n    out_features=out_features,\n    hidden_size=2,\n    number_hidden_layers=number_hidden_layers,\n    device=device,\n)\nmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 5: Training Loop with Growth\n\nNow comes the exciting part! We run an iterative process that alternates\nbetween:\n\n1. **Training**: Standard gradient descent to improve the current network\n2. **Growing**: Adding new neurons to increase capacity\n\n**What to observe:**\n\n- The model starts very small (only 4 hidden neurons, 2 per hidden layer)\n- After each growth step, the model architecture changes (more neurons are\n  added)\n- Test loss should decrease as the network gains capacity\n- The line search output shows how different scaling factors affect\n  performance\n\nWatch how the network progressively grows and improves!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "growth_steps = 4\nintermediate_epochs = 3\n\n# Data collection for plotting\nhistory = {\n    \"step\": [],\n    \"test_loss\": [],\n    \"num_params\": [],\n    \"step_type\": [],  # \"SGD\" or \"GRO\"\n}\n\n\ndef count_parameters(model: torch.nn.Module) -> int:\n    \"\"\"Count the number of trainable parameters in the model.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\"Original model:\")\nprint(model)\n\nlast_test_loss = test_loss = evaluate(model, device, test_data_loader)\nprint(f\"[N/A] Step {0}, Test Loss: {test_loss:.4f}\")\n\n# Record initial state\nhistory[\"step\"].append(0)\nhistory[\"test_loss\"].append(test_loss)\nhistory[\"num_params\"].append(count_parameters(model))\nhistory[\"step_type\"].append(\"SGD\")\n\nfor step in range(growth_steps):\n    for epoch in range(1, intermediate_epochs + 1):\n        train(model, device, train_data_loader)\n        test_loss = evaluate(model, device, test_data_loader)\n        current_step = epoch + step * (intermediate_epochs + 1)\n        print(\n            f\"[SGD] Step {current_step}, \"\n            f\"Test Loss: {test_loss:.4f} ({test_loss - last_test_loss:.4f})\"\n        )\n        last_test_loss = test_loss\n\n        # Record SGD step\n        history[\"step\"].append(current_step)\n        history[\"test_loss\"].append(test_loss)\n        history[\"num_params\"].append(count_parameters(model))\n        history[\"step_type\"].append(\"SGD\")\n\n    layer_to_grow: int = step % max(1, number_hidden_layers) + 1\n    print(f\"Growing layer {layer_to_grow}\")\n    grow(model, device, train_data_loader, layer_to_grow=layer_to_grow)\n    print(\"Model after growing:\")\n    print(model)\n    test_loss = evaluate(model, device, test_data_loader)\n    current_step = (step + 1) * (intermediate_epochs + 1)\n    print(\n        f\"[GRO], Step {current_step}, \"\n        f\"Test Loss: {test_loss:.4f} ({test_loss - last_test_loss:.4f})\"\n    )\n    last_test_loss = test_loss\n\n    # Record growth step (update the last entry to mark it as GRO)\n    history[\"step\"].append(current_step)\n    history[\"test_loss\"].append(test_loss)\n    history[\"num_params\"].append(count_parameters(model))\n    history[\"step_type\"].append(\"GRO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 6: Visualize Training Progress\n\nLet's plot the evolution of the model's performance:\n\n- **Test loss** (left y-axis): Shows how well the model generalizes\n- **Number of parameters** (right y-axis): Shows the model capacity growth\n- **Markers**: Circles (\u25cb) for SGD steps, stars (\u2605) for growth steps\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n\n# Separate data by step type\nsgd_indices = [i for i, t in enumerate(history[\"step_type\"]) if t == \"SGD\"]\ngro_indices = [i for i, t in enumerate(history[\"step_type\"]) if t == \"GRO\"]\n\n# Left y-axis: Test Loss\nax1.set_xlabel(\"Step\", fontsize=12)\nax1.set_ylabel(\"Test Loss\", color=\"tab:blue\", fontsize=12)\nax1.plot(history[\"step\"], history[\"test_loss\"], color=\"tab:blue\", alpha=0.5, linewidth=1)\nax1.scatter(\n    [history[\"step\"][i] for i in sgd_indices],\n    [history[\"test_loss\"][i] for i in sgd_indices],\n    color=\"tab:blue\",\n    marker=\"o\",\n    s=80,\n    label=\"SGD (Loss)\",\n    zorder=3,\n)\nax1.scatter(\n    [history[\"step\"][i] for i in gro_indices],\n    [history[\"test_loss\"][i] for i in gro_indices],\n    color=\"tab:blue\",\n    marker=\"*\",\n    s=200,\n    label=\"Growth (Loss)\",\n    zorder=3,\n)\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# Right y-axis: Number of Parameters\nax2 = ax1.twinx()\nax2.set_ylabel(\"Number of Parameters\", color=\"tab:orange\", fontsize=12)\nax2.plot(\n    history[\"step\"], history[\"num_params\"], color=\"tab:orange\", alpha=0.5, linewidth=1\n)\nax2.scatter(\n    [history[\"step\"][i] for i in sgd_indices],\n    [history[\"num_params\"][i] for i in sgd_indices],\n    color=\"tab:orange\",\n    marker=\"o\",\n    s=80,\n    label=\"SGD (Params)\",\n    zorder=3,\n)\nax2.scatter(\n    [history[\"step\"][i] for i in gro_indices],\n    [history[\"num_params\"][i] for i in gro_indices],\n    color=\"tab:orange\",\n    marker=\"*\",\n    s=200,\n    label=\"Growth (Params)\",\n    zorder=3,\n)\nax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n\n# Combined legend\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n\nplt.title(\"Model Performance and Capacity Evolution\", fontsize=14)\nfig.tight_layout()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}